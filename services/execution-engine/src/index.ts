/**
 * Execution Engine Service Entry Point
 *
 * Reads standby configuration from environment variables and initializes
 * the execution engine with proper failover settings (ADR-007).
 *
 * Environment Variables:
 * - IS_STANDBY: Whether this instance is a standby (default: false)
 * - QUEUE_PAUSED_ON_START: Whether queue starts paused (default: false)
 * - REGION_ID: Region identifier for this instance (default: 'us-east1')
 * - EXECUTION_SIMULATION_MODE: Whether simulation mode is enabled
 *
 * @see ADR-007: Cross-Region Failover Strategy
 */

// Fix 1: Increase max listeners to prevent MaxListenersExceededWarning.
// The execution engine legitimately registers 11+ exit listeners across Redis clients,
// service-bootstrap signal handlers, and graceful shutdown handlers.
process.setMaxListeners(20);

import { IncomingMessage, ServerResponse, Server } from 'http';
import { ExecutionEngineService, SimulationConfig } from './engine';
import { getOrderflowPipelineConsumer } from '@arbitrage/core/analytics';
import { getCrossRegionHealthManager, resetCrossRegionHealthManager } from '@arbitrage/core/monitoring';
import { getErrorMessage } from '@arbitrage/core/resilience';
import {
  setupServiceShutdown,
  closeHealthServer,
  createSimpleHealthServer,
  runServiceMain,
} from '@arbitrage/core/service-lifecycle';
import { parseEnvInt } from '@arbitrage/core/utils';
import { createLogger, parseStandbyConfig } from '@arbitrage/core';
import type { CrossRegionHealthConfig } from '@arbitrage/core/monitoring';
import {
  createCircuitBreakerApiHandler,
} from './api';
import { getMetricsText, updateHealthGauges } from './services/prometheus-metrics';
// P2 Fix DI-6: Import for stream lag monitoring
import { getRedisStreamsClient, RedisStreamsClient } from '@arbitrage/core/redis';

const logger = createLogger('execution-engine');

// Health check port (default: 3005)
const HEALTH_CHECK_PORT = parseInt(process.env.HEALTH_CHECK_PORT || process.env.EXECUTION_ENGINE_PORT || '3005', 10);

let healthServer: Server | null = null;

/**
 * Parse simulation configuration from environment variables.
 * Returns undefined if simulation is not enabled.
 *
 * Exported for unit testing -- not part of the public service API.
 */
export function getSimulationConfigFromEnv(): SimulationConfig | undefined {
  const enabled = process.env.EXECUTION_SIMULATION_MODE === 'true';

  if (!enabled) {
    return undefined;
  }

  return {
    enabled: true,
    successRate: parseFloat(process.env.EXECUTION_SIMULATION_SUCCESS_RATE || '0.85'),
    executionLatencyMs: parseInt(process.env.EXECUTION_SIMULATION_LATENCY_MS || '500', 10),
    gasUsed: parseInt(process.env.EXECUTION_SIMULATION_GAS_USED || '200000', 10),
    gasCostMultiplier: parseFloat(process.env.EXECUTION_SIMULATION_GAS_COST_MULTIPLIER || '0.1'),
    profitVariance: parseFloat(process.env.EXECUTION_SIMULATION_PROFIT_VARIANCE || '0.2'),
    logSimulatedExecutions: process.env.EXECUTION_SIMULATION_LOG !== 'false'
  };
}

/**
 * Parse circuit breaker configuration from environment variables (Phase 1.3).
 *
 * Environment Variables:
 * - CIRCUIT_BREAKER_ENABLED: Whether circuit breaker is enabled (default: true)
 * - CIRCUIT_BREAKER_FAILURE_THRESHOLD: Consecutive failures before tripping (default: 5)
 * - CIRCUIT_BREAKER_COOLDOWN_MS: Cooldown period in ms (default: 300000 = 5 min)
 * - CIRCUIT_BREAKER_HALF_OPEN_ATTEMPTS: Max attempts in HALF_OPEN (default: 1)
 */
/**
 * Exported for unit testing -- not part of the public service API.
 */
export function getCircuitBreakerConfigFromEnv() {
  return {
    enabled: process.env.CIRCUIT_BREAKER_ENABLED !== 'false', // Default: true
    failureThreshold: parseInt(process.env.CIRCUIT_BREAKER_FAILURE_THRESHOLD || '5', 10),
    cooldownPeriodMs: parseInt(process.env.CIRCUIT_BREAKER_COOLDOWN_MS || '300000', 10),
    halfOpenMaxAttempts: parseInt(process.env.CIRCUIT_BREAKER_HALF_OPEN_ATTEMPTS || '1', 10),
  };
}

/**
 * Parse standby configuration from environment variables (ADR-007).
 *
 * Uses shared getCrossRegionEnvConfig for common cross-region fields (S-6).
 * Execution-engine-specific fields (queue pause, standby flag) are parsed here.
 */
/**
 * Exported for unit testing -- not part of the public service API.
 */
export function getStandbyConfigFromEnv() {
  const base = parseStandbyConfig('execution-engine');
  const queuePausedOnStart = process.env.QUEUE_PAUSED_ON_START === 'true';
  return { ...base, queuePausedOnStart };
}

/**
 * Create the execution engine health server using shared createSimpleHealthServer.
 *
 * Endpoints provided:
 * - GET /health — Health check with detailed status (via healthCheck callback)
 * - GET /ready — Readiness check (via readyCheck callback)
 * - GET /stats — Execution statistics (via additionalRoutes)
 * - GET / — Service info (auto-generated by createSimpleHealthServer)
 * - GET /circuit-breaker — Circuit breaker status (via additionalRoutes)
 * - POST /circuit-breaker/close — Force close circuit breaker
 * - POST /circuit-breaker/open — Force open circuit breaker
 */
// Cached Redis health status to avoid blocking health checks with ping on every request
let cachedRedisHealthy = true;
let redisHealthCheckInterval: NodeJS.Timeout | null = null;
// P1-6: Cached DLQ length for health monitoring
let cachedDlqLength = 0;
const DLQ_WARNING_THRESHOLD = 100;
// P2-14: Consumer lag alerting thresholds
let cachedConsumerLag = { pendingCount: 0, minId: null as string | null, maxId: null as string | null };
const CONSUMER_LAG_WARNING_THRESHOLD = 50;

function startRedisHealthMonitor(engine: ExecutionEngineService): void {
  // Check Redis health every 10 seconds and cache the result
  redisHealthCheckInterval = setInterval(async () => {
    cachedRedisHealthy = await engine.isRedisHealthy();
    // P1-6: Monitor DLQ length — alert when failed messages accumulate
    cachedDlqLength = await engine.getDlqLength();
    if (cachedDlqLength > DLQ_WARNING_THRESHOLD) {
      logger.warn('DLQ stream has accumulated failed messages', {
        dlqLength: cachedDlqLength,
        threshold: DLQ_WARNING_THRESHOLD,
        action: 'Investigate failed execution requests in stream:dead-letter-queue',
      });
    }
    // P2-14: Monitor consumer lag — alert when pending messages accumulate
    cachedConsumerLag = await engine.getConsumerLag();
    if (cachedConsumerLag.pendingCount > CONSUMER_LAG_WARNING_THRESHOLD) {
      logger.warn('Consumer lag is high — pending messages accumulating', {
        pendingCount: cachedConsumerLag.pendingCount,
        threshold: CONSUMER_LAG_WARNING_THRESHOLD,
        minId: cachedConsumerLag.minId,
        maxId: cachedConsumerLag.maxId,
        action: 'Check consumer health, increase concurrency, or investigate stalled processing',
      });
    }
    // P2 Fix DI-6: Monitor stream length vs MAXLEN to detect message loss risk
    try {
      const streamsClient = await getRedisStreamsClient();
      const criticalStreams = [
        RedisStreamsClient.STREAMS.EXECUTION_REQUESTS,
        RedisStreamsClient.STREAMS.OPPORTUNITIES,
      ];
      for (const streamName of criticalStreams) {
        const lag = await streamsClient.checkStreamLag(streamName);
        if (lag.critical) {
          logger.warn('Stream length approaching MAXLEN — unread messages at risk of trimming', {
            streamName,
            length: lag.length,
            maxLen: lag.maxLen,
            lagRatio: lag.lagRatio,
          });
        }
      }
    } catch {
      // Stream lag check is non-critical — don't let it break the health monitor
    }
  }, 10_000);
  // Initial check
  engine.isRedisHealthy().then(healthy => { cachedRedisHealthy = healthy; }).catch(() => { cachedRedisHealthy = false; });
}

function createHealthServer(engine: ExecutionEngineService): Server {
  const circuitBreakerHandler = createCircuitBreakerApiHandler(engine);

  return createSimpleHealthServer({
    port: HEALTH_CHECK_PORT,
    serviceName: 'execution-engine',
    logger,
    description: 'Arbitrage Execution Engine Service',
    healthCheck: () => {
      const isRunning = engine.isRunning();
      const stats = engine.getStats();
      const healthyProviders = engine.getHealthyProvidersCount();
      const isSimulation = engine.getIsSimulationMode();

      const status = !isRunning ? 'unhealthy' :
                    (!cachedRedisHealthy) ? 'degraded' :
                    (healthyProviders === 0 && !isSimulation) ? 'degraded' : 'healthy';

      // P2 Fix O-9: Update Prometheus gauges from health endpoint values
      updateHealthGauges({
        queueDepth: engine.getQueueSize(),
        activeExecutions: engine.getActiveExecutionsCount(),
        dlqLength: cachedDlqLength,
        consumerLagPending: cachedConsumerLag.pendingCount,
      });

      return {
        status,
        simulationMode: isSimulation,
        redisConnected: cachedRedisHealthy,
        healthyProviders,
        queueSize: engine.getQueueSize(),
        activeExecutions: engine.getActiveExecutionsCount(),
        executionAttempts: stats.executionAttempts,
        successRate: stats.executionAttempts > 0
          ? (stats.successfulExecutions / stats.executionAttempts * 100).toFixed(2) + '%'
          : 'N/A',
        // P1-6: DLQ monitoring — surface accumulated failed messages
        dlqLength: cachedDlqLength,
        dlqAlert: cachedDlqLength > DLQ_WARNING_THRESHOLD,
        // P2-14: Consumer lag monitoring
        consumerLagPending: cachedConsumerLag.pendingCount,
        consumerLagAlert: cachedConsumerLag.pendingCount > CONSUMER_LAG_WARNING_THRESHOLD,
        uptime: process.uptime(),
        memoryMB: Math.round(process.memoryUsage().heapUsed / 1024 / 1024),
      };
    },
    // P1-9: Ready check verifies engine is running AND essential subsystems are operational.
    // Previously only checked isRunning(), reporting "ready" when Redis was down or no providers healthy.
    // @see docs/reports/EXTENDED_DEEP_ANALYSIS_2026-02-23.md P1-9
    readyCheck: () => {
      if (!engine.isRunning()) return false;
      if (!cachedRedisHealthy) return false;
      // In simulation mode, providers aren't required
      if (!engine.getIsSimulationMode() && engine.getHealthyProvidersCount() === 0) return false;
      return true;
    },
    additionalRoutes: {
      '/metrics': async (_req: IncomingMessage, res: ServerResponse) => {
        const text = await getMetricsText();
        res.writeHead(200, { 'Content-Type': 'text/plain; version=0.0.4; charset=utf-8' });
        res.end(text);
      },
      '/stats': async (_req: IncomingMessage, res: ServerResponse) => {
        const stats = engine.getStats();
        // W2-18 FIX: Include consumer lag metric from XPENDING
        const consumerLag = await engine.getConsumerLag();
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ service: 'execution-engine', stats, consumerLag }));
      },
      // P2-15: Bridge recovery metrics endpoint
      '/bridge-recovery': async (_req: IncomingMessage, res: ServerResponse) => {
        const metrics = engine.getBridgeRecoveryMetrics();
        const isRunning = engine.isBridgeRecoveryRunning();
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ service: 'execution-engine', bridgeRecovery: { isRunning, metrics } }));
      },
      // P2-20: Probability tracker stats endpoint
      '/probability-tracker': async (_req: IncomingMessage, res: ServerResponse) => {
        const stats = engine.getProbabilityTrackerStats();
        res.writeHead(200, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ service: 'execution-engine', probabilityTracker: stats }));
      },
      '/circuit-breaker': circuitBreakerHandler,
      '/circuit-breaker/close': circuitBreakerHandler,
      '/circuit-breaker/open': circuitBreakerHandler,
    },
  });
}

async function main() {
  try {
    const simulationConfig = getSimulationConfigFromEnv();
    const standbyConfig = getStandbyConfigFromEnv();
    const circuitBreakerConfig = getCircuitBreakerConfigFromEnv();

    logger.info(`Starting Execution Engine Service on port ${HEALTH_CHECK_PORT}`);
    logger.debug('Execution engine startup config', {
      simulationMode: simulationConfig?.enabled ?? false,
      isStandby: standbyConfig.isStandby,
      queuePausedOnStart: standbyConfig.queuePausedOnStart,
      regionId: standbyConfig.regionId,
      healthCheckPort: HEALTH_CHECK_PORT,
      circuitBreakerEnabled: circuitBreakerConfig.enabled,
      circuitBreakerThreshold: circuitBreakerConfig.failureThreshold,
    });

    // Generate unique instance ID
    const instanceId = `execution-engine-${standbyConfig.regionId}-${process.env.HOSTNAME || 'local'}-${Date.now()}`;

    const engine = new ExecutionEngineService({
      simulationConfig,
      standbyConfig: {
        isStandby: standbyConfig.isStandby,
        queuePausedOnStart: standbyConfig.queuePausedOnStart,
        activationDisablesSimulation: true, // Default behavior for standby activation
        regionId: standbyConfig.regionId
      },
      circuitBreakerConfig,
    });

    // Initialize CrossRegionHealthManager for cross-region failover (ADR-007)
    // NOTE: Executor only initializes CrossRegionHealthManager when running as standby,
    // unlike Coordinator which always initializes it (coordinator participates in leader
    // election regardless of standby status). This design choice avoids unnecessary
    // overhead for primary executors while ensuring coordinators can always failover.
    let crossRegionManager: ReturnType<typeof getCrossRegionHealthManager> | null = null;
    if (standbyConfig.isStandby) {
      const crossRegionConfig: CrossRegionHealthConfig = {
        instanceId,
        regionId: standbyConfig.regionId,
        serviceName: standbyConfig.serviceName,
        healthCheckIntervalMs: standbyConfig.healthCheckIntervalMs,
        failoverThreshold: standbyConfig.failoverThreshold,
        failoverTimeoutMs: standbyConfig.failoverTimeoutMs,
        leaderHeartbeatIntervalMs: standbyConfig.leaderHeartbeatIntervalMs,
        leaderLockTtlMs: standbyConfig.leaderLockTtlMs,
        canBecomeLeader: true, // Standby executor can become leader on failover
        isStandby: standbyConfig.isStandby
      };

      crossRegionManager = getCrossRegionHealthManager(crossRegionConfig);

      // Wire up failover events
      // P2-4 FIX: Wrap async handler in try-catch to prevent unhandled rejection
      crossRegionManager.on('activateStandby', async (event: { failedRegion: string; timestamp: number }) => {
        try {
          logger.warn('Standby activation triggered by CrossRegionHealthManager', {
            failedRegion: event.failedRegion
          });
          const activated = await engine.activate();
          if (activated) {
            logger.info('Executor successfully activated');
          } else {
            logger.error('Failed to activate executor');
          }
        } catch (error) {
          logger.error('Error during standby activation', {
            error: getErrorMessage(error),
            failedRegion: event.failedRegion
          });
        }
      });

      crossRegionManager.on('failoverStarted', (event) => {
        logger.warn('Failover started', {
          sourceRegion: event.sourceRegion,
          targetRegion: event.targetRegion,
          services: event.services
        });
      });

      crossRegionManager.on('failoverCompleted', (event) => {
        logger.info('Failover completed', {
          sourceRegion: event.sourceRegion,
          targetRegion: event.targetRegion,
          durationMs: event.durationMs
        });
      });

      // Start cross-region health manager
      await crossRegionManager.start();
      logger.info('CrossRegionHealthManager started for standby executor');
    }

    // Start health server first
    healthServer = createHealthServer(engine);
    startRedisHealthMonitor(engine);

    await engine.start();

    // Start orderflow pipeline consumer (no-op if FEATURE_ORDERFLOW_PIPELINE != true)
    const orderflowConsumer = getOrderflowPipelineConsumer();
    await orderflowConsumer.start();

    // Graceful shutdown with shared bootstrap utility
    setupServiceShutdown({
      logger,
      serviceName: 'Execution Engine',
      onShutdown: async () => {
        // Stop cross-region health manager if running
        // P1-2 FIX: Remove event listeners before destroying manager to prevent memory leak
        if (crossRegionManager) {
          crossRegionManager.removeAllListeners();
          await resetCrossRegionHealthManager();
        }

        if (redisHealthCheckInterval) {
          clearInterval(redisHealthCheckInterval);
          redisHealthCheckInterval = null;
        }
        await orderflowConsumer.stop();
        await closeHealthServer(healthServer);
        await engine.stop();
      },
    });

    logger.info('Execution Engine Service is running');

  } catch (error) {
    logger.error('Failed to start Execution Engine Service', { error });
    process.exit(1);
  }
}

runServiceMain({ main, serviceName: 'Execution Engine Service', logger });
