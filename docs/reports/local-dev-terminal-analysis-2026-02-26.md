# Local Dev Test Run ‚Äî Terminal Output Analysis

**Date:** 2026-02-26  
**Duration:** ~15 min (15:36 ‚Üí 15:52 UTC)  
**Services Analyzed:** Coordinator, P1 (asia-fast), P2 (l2-turbo), Cross-Chain Detector, Execution Engine  
**Total log lines:** ~52,600

---

## Executive Summary

The local dev test run reveals **8 critical issues** and **6 notable enhancements** needed before any production deployment. The most severe is a whale alert spam pipeline that generated **1,674 false-positive signals** from simulated swap data, flooding the cross-chain detector and coordinator. The coordinator oscillated between degradation levels **55 times** in 15 minutes due to a health-check timing bug. The execution engine sat completely idle ‚Äî zero requests processed ‚Äî indicating the opportunity-to-execution pipeline is broken.

---

## üî¥ Critical Issues

### 1. Whale Alert Spam from Simulated Data (False Positives)

> [!CAUTION]
> The P1 (asia-fast) partition runs in **simulation mode** but publishes whale alerts to Redis as if they were real on-chain events. These simulated swaps are then broadcast identically to **all 4 chains** (bsc, polygon, avalanche, fantom), creating a multiplicative spam effect.

| Metric | Count |
|--------|------:|
| Whale alerts published by P1 | **1,684** |
| Whale alerts received by Coordinator | **99** (sampled, not all consumed) |
| Whale signals generated by Cross-Chain Detector | **1,674** |
| Super whale scans triggered | **356** |
| Unique whale addresses | **2** (`0x10ed43c...`, `0x13f4ea8...`) |

**Root Cause:** The chain simulator in P1 generates simulated swap events that pass through `SwapEventFilter` without a simulation-mode gate. Each whale swap on one simulated chain (e.g., bsc) is then re-published to polygon, avalanche, and fantom ‚Äî **quadrupling** the alert volume.

**Evidence ‚Äî Same whale, same amount, 4 different chains:**
```
[15:41:26] Whale alert: 0x10ed... $15,650,014 buy on bsc
[15:41:33] Whale alert: 0x10ed... $15,650,014 buy on polygon
[15:41:40] Whale alert: 0x10ed... $15,650,014 buy on avalanche
[15:41:46] Whale alert: 0x10ed... $15,650,014 buy on fantom
```

**Fix:** Add a `simulationMode` guard to suppress whale alert publishing in simulation, or tag alerts with `source: "simulation"` and filter downstream.

---

### 2. Health-Check Oscillation Bug (Coordinator)

> [!WARNING]
> The coordinator flipped degradation status **55 times** in 15 minutes ‚Äî oscillating between `FULL_OPERATION ‚Üí READ_ONLY ‚Üí COMPLETE_OUTAGE` every 5-10 seconds.

**Root Cause:** The coordinator's health-check interval (5s) races against the heartbeat threshold (30s). When a stale heartbeat from one service is detected, it flips to `COMPLETE_OUTAGE`. The next tick, a different service's heartbeat arrives, flipping back to `READ_ONLY`. This creates a flapping pattern:

```
[15:39:54] COMPLETE_OUTAGE (systemHealth: 0)
[15:40:04] READ_ONLY       (systemHealth: 100)    ‚Üê 10s later
[15:40:09] COMPLETE_OUTAGE (systemHealth: 0)      ‚Üê 5s later
[15:40:14] READ_ONLY       (systemHealth: 100)    ‚Üê 5s later
```

**Fix:**
- Add **hysteresis / dampening** ‚Äî require N consecutive unhealthy checks before downgrading
- Add a **debounce** on degradation-level changes (e.g., minimum 30s between transitions)
- Separate self-health vs. peer-health evaluation: the coordinator reported *itself* as unhealthy

---

### 3. Execution Engine Receives Zero Work

The execution engine ran for **~10 minutes** and processed **zero** execution requests despite P1 detecting **2,940 arbitrage opportunities**.

```
simulationsPerformed: 0
simulationsSkipped: 0
transactionsSkippedBySimulation: 0
```

**Root Cause:** The opportunities are published to `stream:opportunities` but the execution engine consumes from `stream:execution-requests`. The coordinator should be forwarding validated opportunities to the execution stream ‚Äî but it was stuck oscillating between degradation levels and never reached `FULL_OPERATION` status to forward requests.

**Fix:** Ensure the coordinator forwards opportunities to `stream:execution-requests` even in `DETECTION_ONLY` mode, or allow the execution engine to optionally consume directly from `stream:opportunities` when the coordinator is unhealthy.

---

### 4. Unrealistic Simulated Profit Percentages

P1's simulator generated opportunities with **impossible profit margins** that would never exist on-chain:

| Percentage Range | Count |
|------------------|------:|
| 100% ‚Äì 200%     | ~1,800 |
| 200% ‚Äì 500%     | ~800 |
| > 451%           | 27 |
| > 499%           | 27 |

Example: `pancakeswap_v3 ‚Üí biswap` at **499.65%** profit. Real-world arbitrage profits are typically 0.01%‚Äì2%.

**Impact:** These unrealistic values will train the ML predictor on garbage data and produce meaningless EV calculations.

**Fix:** Cap the simulator's price variance to produce realistic spreads (0.01% ‚Äì 5%), or add a sanity-check filter on published opportunities.

---

### 5. Invalid Price Update Messages (Cross-Chain Detector)

The cross-chain detector skipped **53 invalid price update messages** from the P1 partition.

```
[15:43:32] WARN: Skipping invalid price update message (messageId: 1772120254956-0)
[15:43:33] WARN: Skipping invalid price update message (messageId: 1772120254956-1)
...
```

**Root Cause:** Likely a schema mismatch between P1's price-update publisher and the cross-chain detector's consumer. The messages at IDs `1772120254956-*` through `1772120255068-*` were rejected in a burst ‚Äî suggesting a batch of malformed messages from P1's initial price snapshot.

**Fix:** Add schema validation logging to show *why* messages are invalid (missing fields, wrong types). Investigate whether the P1 simulator's initial price broadcast uses the correct message format.

---

### 6. Ankr WebSocket 401 Authentication Errors (P2)

Both Scroll and Optimism WebSocket connections to Ankr failed with HTTP 401:

```
Unexpected server response: 401
URL: wss://rpc.ankr.com/scroll/fd86c2f5d5ff...
URL: wss://rpc.ankr.com/optimism/fd86c2f5d5ff...
```

**Impact:** Scroll lost **15 blocks** of data, Optimism lost **6 blocks** during reconnection.

**Fix:** The Ankr API key (`fd86c2f5...`) appears to be expired or rate-limited. Rotate the key, or remove Ankr from the fallback rotation if the free tier doesn't support WebSockets. The provider-rotation system correctly fell back to `publicnode.com`, but wasted ~10s per chain in the process.

---

### 7. Destroyed Batcher Warnings (P1)

**1,612** messages attempted to write to a destroyed Redis batcher on `stream:price-updates`.

```
[15:52:01] WARN: Attempted to add message to destroyed batcher (stream: stream:price-updates)
```

**Root Cause:** The Redis Streams batcher is destroyed during shutdown, but the chain simulator continues generating price updates that try to queue. This is a race condition in the shutdown sequence.

**Fix:** Stop the chain simulators *before* destroying the Redis batcher. Add a `isStopping` flag that simulators check before publishing.

---

### 8. Redis EPIPE / ECONNREFUSED at Shutdown

All 5 services experienced the same Redis failure sequence at `15:52:12`:

```
1. Redis main client closed
2. Redis main client connected     ‚Üê reconnects
3. write EPIPE                     ‚Üê pipe broken
4. Redis main client closed
5. ECONNREFUSED 127.0.0.1:6379    ‚Üê Redis actually gone
6. Redis Streams: connection failed after 3 retries
```

**Root Cause:** Redis was stopped before the services. ioredis auto-reconnects, hits the broken pipe, then gets ECONNREFUSED. The `tsx watch` process had to be **force-killed** (`Previous process hasn't exited yet. Force killing...`).

**Fix:** Implement a proper shutdown sequence: send SIGTERM to services first, wait for graceful shutdown, *then* stop Redis. Also set `maxRetriesPerRequest: 1` during shutdown to avoid the 3-retry delay.

---

## üü° Configuration Mismatches & Warnings

### A. MaxListenersExceededWarning (All Services)

Every service emits:
```
MaxListenersExceededWarning: 11 exit listeners added to [process]. MaxListeners is 10.
```

**Fix:** Call `process.setMaxListeners(15)` in each service's entry point, or audit which modules are adding exit listeners to avoid the accumulation.

### B. Missing Notification Channels (Coordinator)

```
No alert notification channels configured. Set DISCORD_WEBHOOK_URL or SLACK_WEBHOOK_URL
```

Every alert logs twice ‚Äî once as a trigger, once as "no notification channels". This doubles log volume for alerts.

### C. Unprotected API Endpoints (Coordinator)

```
API authentication NOT configured - endpoints are unprotected
authEnabled: false
```

The coordinator binds to `0.0.0.0` with no authentication ‚Äî anyone on the network can hit its API.

### D. TensorFlow.js Performance Warning (Cross-Chain)

```
Orthogonal initializer is being called on a matrix with more than 2000 (65536) elements: Slowness may result.
```

The LSTM model takes **20 seconds** to initialize (`[15:43:01]` ‚Üí `[15:43:21]`). The native TensorFlow Node backend should be installed: `npm install @tensorflow/tfjs-node`.

### E. CommitRevealService Has No Contracts (Execution)

```
CommitRevealService initialized with no deployed contracts. All commit-reveal operations will fail.
```

All 10 chain contract addresses are empty strings. This is expected for dev but would be a production blocker.

### F. Reserve Cache 0% Hit Rate (P1 & P2)

Across the entire run, both partition services logged **0.0% hit rate** on their reserve caches. P1 had 441 sync updates but zero actual reads. The cache is populated but never consulted ‚Äî the detection pipeline bypasses it.

---

## üü¢ Enhancement Recommendations

| # | Enhancement | Priority | Effort |
|---|-------------|----------|--------|
| 1 | **Rate-limit whale alert publishing** ‚Äî Deduplicate by `(address, amount, direction)` within a 30s window | High | Low |
| 2 | **Add simulation tag to all simulated events** ‚Äî `source: "simulation"` field in stream messages | High | Low |
| 3 | **Install `@tensorflow/tfjs-node`** for native LSTM performance | Medium | Low |
| 4 | **Implement health-check dampening** ‚Äî require 3+ consecutive failures before changing degradation level | High | Medium |
| 5 | **Add graceful shutdown orchestration** ‚Äî stop producers before consumers, consumers before Redis | High | Medium |
| 6 | **Fix opportunity ‚Üí execution pipeline** ‚Äî coordinator should forward to execution stream in DETECTION_ONLY mode | Critical | Medium |

---

## Service Timeline

```
15:36:12  Coordinator starts (1st attempt)
15:37:31  P1 (asia-fast) starts ‚Äî 4 chains in simulation mode
15:37:39  P1 starts generating arbitrage opportunities
15:38:12  Coordinator crashes (Redis ECONNREFUSED) ‚Äî 1st instance lost leadership
15:39:32  Coordinator restarts (2nd attempt)
15:40:58  P2 (l2-turbo) starts ‚Äî 5 chains in production mode
15:41:26  First whale alert received by coordinator
15:42:09  Execution engine starts (simulation mode)
15:42:51  P2: Scroll WebSocket disconnects, Ankr fallback fails (401)
15:43:00  Cross-chain detector starts ‚Äî TensorFlow LSTM loads
15:43:21  Cross-chain detector begins processing whale alert backlog (1,674 signals)
15:50:15  P2: Optimism WebSocket disconnects, Ankr fallback fails (401)
15:52:12  Redis stops ‚Äî all 5 services cascade-fail with ECONNREFUSED
15:52:13  tsx force-kills all processes
```

---

## Files Analyzed

| File | Lines | Size |
|------|------:|-----:|
| `output_coordinator.txt` | 2,577 | 93 KB |
| `output_p1_service.txt` | 36,638 | 1.3 MB |
| `output_p2_service.txt` | 625 | 27 KB |
| `output_cross_chain_service.txt` | 12,166 | 409 KB |
| `output_execution_service.txt` | 620 | 22 KB |
