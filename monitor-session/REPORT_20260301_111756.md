# Monitoring Session Report

**Session ID:** `20260301_111756`
**Generated:** 2026-03-01T10:35:00Z
**Protocol:** MONITORING.md v1.0

---

## 1. Executive Dashboard

| Metric | Value |
|---|---|
| Session ID | `20260301_111756` |
| Session duration | ~12 minutes (10:21:57Z — 10:33:49Z) |
| Services monitored | 7 (coord, p1, p2, p3, p4, cross, exec) |
| Streams discovered | 12 |
| Total findings | **61** (28 Log Watcher + 20 Config Auditor + 13 Enhancement Scout) |
| CRITICAL findings | **4** |
| HIGH findings | **16** |
| MEDIUM findings | **16** |
| LOW/INFO findings | **11** |
| Unacked messages at shutdown | 27 (9 opportunities + 10 price-updates + 4 health + 3 execution-requests + 1 cross-chain) |
| Streams with consumer lag | **3** (opportunities: 10000, health: 243, price-updates: coordinator stalled) |
| Config/Doc drifts found | 8 |
| Overall system health | CRITICAL |

---

## 2. Critical Findings (Immediate Action Required)

### CRITICAL-1: Thundering Herd Production Crisis [ES-009 + LW-011 + LW-026]

**Agents:** Enhancement Scout (synthesis), Log Watcher
**Service:** partition-high-value (P3)
**Streams:** stream:opportunities

**Evidence:**
- P3 detected **122,634 opportunities in 8 minutes** = **255 opps/sec sustained**
- Concurrent publish limit reached **90,674 times** = 74% of opportunities dropped at publish stage
- stream:opportunities at **MAXLEN capacity** (10,000/10,000) — messages being trimmed
- coordinator-group has **lag of 10,000** = consumer is at maximum lag capacity
- No circuit breaker exists on `xadd()` (ES-002 predicted this exact failure mode)

**Root Cause Chain:**
```
Detection threshold too low (P3 MIN_PROFIT_THRESHOLD)
  → 255 opps/sec generated (10x sustainable rate)
    → XADD concurrent limit saturated (100 in-flight cap)
      → No circuit breaker → infinite retry storm
        → Stream hits MAXLEN → old messages trimmed before consumption
          → 95% opportunity loss rate
```

**Blast Radius:** P3 is generating so much work that it dominates all system resources. Log volume reaches 1.27M lines / 422KB/sec in 8 minutes. Execution engine cannot keep up. Dead letter queue grows continuously.

**Fix (priority order):**
1. **Immediate:** Increase `MIN_PROFIT_THRESHOLD` for P3 by 10x to reduce detection rate
2. **Urgent:** Add circuit breaker to `xadd()` in `shared/core/src/redis/streams.ts:572`
3. **Short-term:** Scale out execution consumers or increase `MAX_CONCURRENT_PUBLISHES`
4. **Medium-term:** Implement backpressure protocol (ES-013)

---

### CRITICAL-2: Linea RPC Provider in Infinite Retry Loop [LW-004 + LW-012]

**Agent:** Log Watcher
**Service:** partition-high-value (P3)

**Evidence:**
- `JsonRpcProvider failed to detect network and cannot start up; retry in 1s`
- **133 consecutive failures** over 6+ minutes at 1 retry/second
- Related: SSL certificate error (`UNABLE_TO_GET_ISSUER_CERT_LOCALLY`) on multiple adapters (balancer_v2, beethoven_x, gmx, platypus)
- Gas price for Linea reported **stale (181s old)** due to failed provider

**Root Cause:** RPC_URL_LINEA either misconfigured or blocked by corporate proxy/SSL inspection. No circuit breaker on RPC provider initialization — service wastes resources retrying indefinitely.

**Fix:**
1. Verify `RPC_URL_LINEA` in `.env` — test endpoint manually
2. If corporate proxy: add `NODE_TLS_REJECT_UNAUTHORIZED=0` to `.env.local`
3. Add RPC provider circuit breaker (threshold=5, timeout=60s) to fail fast
4. P3 shows adaptive behavior (LW-023): shifted from Linea to Ethereum-only detection — but still retrying Linea in background

---

### CRITICAL-3: Dead Letter Queue Growing Unbounded [LW-005 + LW-028 + ES-010]

**Agents:** Log Watcher, Enhancement Scout
**Service:** execution-engine
**Stream:** stream:dead-letter-queue

**Evidence:**
- DLQ grew from **275 → 323** during session (+48 messages)
- `DLQ persistently above threshold (dlqLength: 283, threshold: 100, consecutiveBreaches: 56)`
- Messages are **3+ minutes old** (193,927ms) before reaching execution
- DLQ has **NO consumer groups** — messages accumulate indefinitely
- No `maxDelivery` enforcement in `StreamConsumer` (ES-003) — failed messages loop forever

**Root Cause:** Opportunities expire before execution due to overloaded pipeline. Expired messages moved to DLQ but never drained. System is in a death spiral: more opportunities → more expiration → larger DLQ → more warnings.

**Fix:**
1. Add `maxDeliveryCount=3` to StreamConsumerConfig
2. Add TTL-based expiry: if message age > 60s, XACK without retry
3. Add DLQ auto-drain: if DLQ > 200, bulk XACK oldest 100
4. Create consumer group for DLQ to process/archive dead letters

---

### CRITICAL-4: 5 Streams Silently Idle — No Traffic Flowing [Orchestrator]

**Agent:** Orchestrator observation
**Streams:** stream:pending-opportunities (0), stream:system-failover (0), stream:swap-events (0), stream:whale-alerts (0), stream:volume-aggregates (0)

**Evidence:** These 5 streams remained at length=0 for the entire 12-minute session. They have consumer groups registered but no messages were ever published. Additionally, `stream:service-degradation` (8 messages) and `stream:dead-letter-queue` (323 messages) have **no consumer groups at all**.

**Significance:** Either these features are not enabled in dev mode (acceptable) or there is a configuration gap preventing data flow. The 2 streams without consumers represent a data sink — messages published but never processed.

---

## 3. Redis Streams Health Map

| Stream | Baseline | Final | Delta | Pending | Lag | Consumer Groups | Status |
|---|---|---|---|---|---|---|---|
| `stream:opportunities` | 10,002 | 10,000 | -2 | 9 | **10,000** | coordinator-group (8 consumers) | CRITICAL — at MAXLEN, max lag |
| `stream:price-updates` | 19,040 | 19,479 | +439 | 11 | ~null/1 | coordinator-group, cross-chain-detector-group | DEGRADED — coordinator stalled |
| `stream:execution-requests` | 231 | 280 | +49 | 3 | 1 | execution-engine-group (7 consumers) | HEALTHY |
| `stream:dead-letter-queue` | 275 | 323 | +48 | 0 | N/A | **NONE** | CRITICAL — no consumers, growing |
| `stream:health` | 1,002 | 1,003 | +1 | 4 | **243** | coordinator-group (8 consumers) | DEGRADED — significant lag |
| `stream:service-degradation` | 8 | 8 | 0 | 0 | N/A | **NONE** | WARNING — no consumers |
| `stream:execution-results` | 1 | 1 | 0 | 0 | 0 | coordinator-group (8 consumers) | HEALTHY (idle) |
| `stream:pending-opportunities` | 0 | 0 | 0 | 0 | 0 | 2 groups (6+4 consumers) | HEALTHY (idle) |
| `stream:system-failover` | 0 | 0 | 0 | 0 | 0 | failover-coordinator (8) | HEALTHY (idle) |
| `stream:swap-events` | 0 | 0 | 0 | 0 | 0 | coordinator-group (8) | HEALTHY (idle) |
| `stream:whale-alerts` | 0 | 0 | 0 | 0 | 0 | 2 groups | HEALTHY (idle) |
| `stream:volume-aggregates` | 0 | 0 | 0 | 0 | 0 | coordinator-group (8) | HEALTHY (idle) |

**Summary:** 2 CRITICAL, 2 DEGRADED, 1 WARNING, 7 HEALTHY (5 idle)

**Key observations:**
- `stream:opportunities` is at MAXLEN (10,000) with coordinator-group lagging by the full stream length — coordinator cannot keep up with partition publishing rate
- `stream:price-updates` grew +439 messages (most active stream) — cross-chain-detector keeping up (lag=1), but coordinator-group appears stalled (pending=10, lag=null)
- `stream:dead-letter-queue` and `stream:service-degradation` have **no consumer groups** — published data is never processed

---

## 4. Cross-Service Event Flow Reconstruction

Based on log correlation and stream message analysis during the session:

### Primary Flow: Opportunity Detection → Coordination → Execution

```
[T+0:00] P3 (partition-high-value) detects opportunities on Ethereum/zkSync/Linea
         → 255 opps/sec sustained rate
         → XADD to stream:opportunities (MAXLEN ~10000)
         → 74% DROPPED at publish (concurrent limit=100)

[T+0:00] P1/P2/P4 also detect opportunities (much lower rate)
         → XADD to stream:opportunities

[T+0:01] Coordinator reads stream:opportunities (coordinator-group)
         → LAG: 10,000 messages behind
         → Processing rate << production rate
         → 9 messages pending (being processed)

[T+0:02] Coordinator forwards valid opps → stream:execution-requests
         → +49 messages during session

[T+0:03] Execution Engine reads stream:execution-requests (execution-engine-group)
         → LAG: 1 message (keeping up with coordinator output)
         → 3 messages pending
         → Messages arrive 3+ minutes old → expire → moved to DLQ

[T+0:04] DLQ accumulates (+48 during session, no consumers)
```

### Secondary Flow: Price Updates → Cross-Chain Detection

```
[T+0:00] Partition services publish price updates
         → XADD to stream:price-updates (+439 during session)

[T+0:01] Cross-chain-detector reads (cross-chain-detector-group)
         → LAG: 1 message (keeping up well)
         → Calculates cross-chain amounts
         → WARN: Token amounts exceed maximum, capping (10T+ amounts)
         → Publishes to stream:pending-opportunities

[T+0:01] Coordinator reads stream:price-updates (coordinator-group)
         → LAG: STALLED (pending=10, lag=null)
         → Coordinator may be overwhelmed by opportunity stream processing
```

### Broken Flows (messages published but never consumed):

| Published To | Producer | Consumer Group | Status |
|---|---|---|---|
| `stream:service-degradation` | graceful-degradation | NONE | Messages lost |
| `stream:dead-letter-queue` | execution-engine | NONE | Accumulating forever |
| `stream:service-recovery` | graceful-degradation | DOES NOT EXIST | Publish to void |
| `stream:system-failures` | expert-self-healing | DOES NOT EXIST | Publish to void |
| `stream:system-control` | expert-self-healing | DOES NOT EXIST | Publish to void |
| `stream:system-scaling` | expert-self-healing | DOES NOT EXIST | Publish to void |

---

## 5. Config & Documentation Drift Registry

| # | Type | Service | Expected | Actual | Source | Severity |
|---|---|---|---|---|---|---|
| CA-001 | ROGUE_STREAM | cross-service | 12 runtime streams | Code declares 3 non-existent streams (system-failures, system-control, system-scaling) | expert-self-healing-manager.ts:29-32 | HIGH |
| CA-002 | ROGUE_STREAM | cross-service | stream:service-degradation | Code publishes to stream:service-recovery (doesn't exist) | graceful-degradation.ts:485 | MEDIUM |
| CA-004 | DOC_DRIFT | cross-service | Runtime has 12 streams | events.ts declares 8 additional phantom streams (service-health, service-events, coordinator-events, etc.) | shared/types/src/events.ts | LOW |
| CA-005 | GROUP_NAME_MISMATCH | execution-engine | Consistent group naming | fast-lane.consumer uses 'execution-engine', opportunity.consumer uses 'execution-engine-group' | fast-lane.consumer.ts:101 | MEDIUM |
| CA-009 | MISSING_ENV_VAR | cross-service | All env vars documented | 201 of 322 env vars undocumented (62%) | .env.example | MEDIUM |
| CA-010 | SCHEMA_DRIFT | cross-service | All declared streams either exist or documented as on-demand | 3 streams declared but never created | expert-self-healing-manager.ts | HIGH |
| CA-013 | PORT_CONFLICT | docker-partitions | Ports match documented service ports | All partitions map internal port 3001 regardless of service | docker-compose.partitions.yml | HIGH |
| CA-020 | DOC_DRIFT | cross-service | ADR-002 documents all streams | ADR-002 covers only 2 of 12 operational streams (17%) | ADR-002-redis-streams.md | HIGH |

**Additional drifts:** CA-003 (no stream schema docs), CA-006/CA-007/CA-008 (hardcoded vs constant stream names), CA-011 (degradation/recovery naming), CA-012 (no centralized stream registry), CA-014 (HEALTH_CHECK_PORT mismatch in Docker), CA-016 (unused feature flags), CA-017/CA-018 (parallel constant hierarchies)

---

## 6. Enhancement Roadmap

### P0 — Fix Before Next Deploy

**ES-009: Thundering Herd Crisis** (DAYS_1_2, HIGH impact)
- P3 generating 255 opps/sec, 74% dropped
- Root cause: no circuit breaker + detection threshold too low
- Fix: circuit breaker on `xadd()`, increase MIN_PROFIT_THRESHOLD 10x
- Files: `shared/core/src/redis/streams.ts:572`, P3 partition config

### P1 — Fix This Sprint

**ES-002: Circuit Breaker on XADD Producers** (HOURS_4_8, HIGH impact)
- All partition services hammer Redis during outages with exponential backoff
- No fail-fast mechanism
- File: `shared/core/src/redis/streams.ts:572-634`
- Add `createSimpleCircuitBreaker({threshold: 5, resetTimeoutMs: 30000})`

**ES-010: DLQ Overload — maxDelivery + Auto-Drain** (HOURS_4_8, HIGH impact)
- DLQ at 323 messages, no consumers, growing at 48/session
- No maxDelivery enforcement in StreamConsumer
- Files: `shared/core/src/redis/stream-consumer.ts:203`, `shared/core/src/resilience/dead-letter-queue.ts`
- Add `maxDeliveryCount=3`, TTL-based expiry, DLQ auto-drain

**ES-006: Lag Monitoring Implementation** (DAYS_1_2, HIGH impact)
- `checkStreamLag()` referenced in comments but not implemented
- stream:opportunities at MAXLEN with lag=10,000 and no alert
- File: `shared/core/src/redis/streams.ts:640`
- Implement XPENDING-based lag check, alert at 80% of MAXLEN

### P2 — Backlog (Prioritized)

| # | Enhancement | Effort | Impact | Files |
|---|---|---|---|---|
| ES-005 | Stream Batcher Prometheus metrics | HOURS_4_8 | HIGH | `shared/core/src/redis/streams.ts:133` |
| ES-007 | Correlation ID propagation from detector | HOURS_2_4 | MEDIUM | `shared/core/src/publishers/opportunity-publisher.ts:92` |
| ES-011 | RPC provider circuit breaker (Linea stuck) | HOURS_2_4 | MEDIUM | Partition service RPC initialization |
| ES-013 | Backpressure protocol (producer ↔ consumer) | DAYS_1_2 | MEDIUM | New ADR needed |
| ES-001 | Replace console.log fallback in Solana detector | HOURS_2_4 | MEDIUM | `services/partition-solana/src/arbitrage-detector.ts:257` |
| ES-003 | maxDelivery config in StreamConsumer | HOURS_2_4 | MEDIUM | `shared/core/src/redis/stream-consumer.ts` |

### P3 — Low Priority

| # | Enhancement | Effort | Impact |
|---|---|---|---|
| ES-004 | Configurable batch sizes per stream | HOURS_2_4 | LOW |
| ES-008 | Type-safe stream names (StreamName union type) | HOURS_2_4 | LOW |
| ES-012 | Stream schema drift cleanup | HOURS_2_4 | LOW |

---

## 7. Logging & Observability Gap Analysis

### What Was Invisible During This Session

**A. No correlation ID tracing across services (ES-007)**
- Opportunities flow from P3 → coordinator → execution-engine, but there is no way to trace a specific opportunity through the entire pipeline
- `OpportunityPublisher.publish()` creates NEW trace context instead of propagating detector's context
- Impact: When execution fails, there's no way to trace back to which price update triggered the detection
- Fix: `shared/core/src/publishers/opportunity-publisher.ts:92` — accept parent `TraceContext` parameter

**B. No consumer lag metrics exported (ES-005 + ES-006)**
- `stream:opportunities` was at MAXLEN with lag=10,000 during the entire session
- No Prometheus metric exposed this — the only signal was a WARN log buried in millions of lines
- `BatcherStats` interface exists but isn't exported to Prometheus
- Fix: Implement `checkStreamLag()`, export `redis_batcher_*` and `redis_consumer_lag` Prometheus metrics

**C. DLQ has no consumer — dead letters are invisible**
- 323 messages sit in `stream:dead-letter-queue` with no consumer group
- No alerting, no processing, no archive
- Fix: Create DLQ consumer group, add Prometheus metric `dlq_messages_total{stream}`

**D. Service degradation events never processed**
- `stream:service-degradation` has 8 messages but no consumer group
- Graceful degradation publishes events, but nothing reads them
- Fix: Add consumer in coordinator to act on degradation signals

**E. 5 idle streams — unknown if intentional**
- `pending-opportunities`, `system-failover`, `swap-events`, `whale-alerts`, `volume-aggregates` all at length=0
- Consumer groups exist but no messages published
- Cannot determine if features are disabled, not configured, or broken

**F. Log volume makes analysis impossible at scale**
- 1.27M log lines in 8 minutes (2,639 lines/sec, 422KB/sec)
- P3 generates 95% of all log output
- Manual log analysis is not feasible — structured logging with filtering is required
- Fix: Reduce P3 log level to WARN, implement sampling for opportunity logs, add structured JSON format

### Priority Instrumentation for Next Session

1. **Correlation IDs (P0):** Every Redis Stream event payload must carry `correlationId`, `traceId`, `originService`, `publishedAt`, `schemaVersion`. Implement `createEventEnvelope()` utility.

2. **Structured JSON Logging (P1):** All services must emit `{level, timestamp, service, correlationId, traceId, event, message, durationMs, error}`. Identify services using `console.log` (partition-solana confirmed).

3. **Redis Stream Lifecycle Events (P1):** Auto-instrument `redis.publish`, `redis.consume`, `redis.ack`, `redis.failed` events in the Redis wrapper.

4. **Consumer Lag Health Reporting (P2):** Each consumer emits `redis.channelHealth` every 60s with `messagesConsumedLastMinute`, `avgLagMs`, `maxLagMs`, `pendingCount`.

5. **Startup & Shutdown Events (P2):** Every service emits `service.started` with `subscribedStreams`, `publishedStreams`, `consumerGroups` and `service.shutdown` with `pendingMessages`.

---

## 8. Next Session Improvements

Before running this monitoring session again, implement these changes (in priority order):

| # | Improvement | Gap Closed | Agent Benefiting | Effort |
|---|---|---|---|---|
| 1 | **Reduce P3 detection threshold 10x** | Eliminates thundering herd, reduces log noise 90% | Log Watcher, Stream Analyst | 15 min |
| 2 | **Add consumer group for stream:dead-letter-queue** | DLQ becomes observable | Stream Analyst | 30 min |
| 3 | **Add consumer group for stream:service-degradation** | Degradation events become visible | Log Watcher | 30 min |
| 4 | **Fix Linea RPC URL or disable chain** | Eliminates 133+ retry-loop noise | Log Watcher | 15 min |
| 5 | **Set NODE_TLS_REJECT_UNAUTHORIZED=0 in .env.local** | Eliminates 18+ SSL cert errors per session | Log Watcher | 5 min |
| 6 | **Implement checkStreamLag() with Prometheus export** | Stream Analyst can detect lag trends | Stream Analyst, Enhancement Scout | 4 hours |
| 7 | **Add circuit breaker to xadd()** | Prevents thundering herd recurrence | Stream Analyst, Log Watcher | 4 hours |
| 8 | **Reduce P3 log level to WARN in dev** | Reduces log volume 90%, improves analysis | Log Watcher | 15 min |
| 9 | **Add correlation ID propagation** | Enables end-to-end trace reconstruction | All agents | 8 hours |
| 10 | **Update ADR-002 with full stream registry** | Config Auditor has authoritative baseline | Config Auditor | 2 hours |

### Agent-Specific Notes

**Redis Stream Analyst:** This agent failed to produce findings due to Node.js Redis helper parsing issues (no `redis-cli` binary available on Windows). For next session: either install Redis CLI tools or improve the `redis-cmd.js` helper to handle XINFO STREAM response format correctly. The orchestrator captured stream health data as a fallback.

**Log Watcher:** Highly effective — produced 28 meaningful findings. The main challenge was log volume (1.27M lines). For next session, consider piping service logs to separate files per service for more targeted analysis.

**Config Auditor:** Effective at finding schema drift and documentation gaps. Found the 55% phantom stream rate (13 streams in code but not in runtime). Recommend running this audit as part of CI/CD.

**Enhancement Scout:** Cross-agent synthesis was the most valuable output. ES-009 (thundering herd synthesis) correctly identified the root cause by connecting Log Watcher's retry warnings with its own static analysis of missing circuit breakers.

---

## Appendix: Finding Index

### By Severity

**CRITICAL (4):**
- LW-004: Linea RPC infinite retry loop
- LW-011: 49,579 concurrent publish limit warnings
- LW-026: 255 opps/sec sustained overload (122K in 8min)
- ES-009: Thundering herd synthesis (P3 + no circuit breaker + consumer lag)

**HIGH (16):**
- LW-001: EventEmitter memory leak (MaxListeners exceeded)
- LW-003: Concurrent publish limit saturated (100 in-flight)
- LW-005: DLQ above threshold (283/100, 20 breaches)
- LW-012: Linea RPC 133 consecutive failures
- LW-013: DLQ 23 breaches in 6 minutes
- LW-020: P3 detecting without live Linea RPC
- LW-021: 95K opportunities in 6 minutes
- LW-027: 1.27M log lines (422KB/sec)
- LW-028: DLQ degradation worsening over time
- CA-001: 3 rogue streams in expert-self-healing-manager
- CA-010: Schema drift (3 declared-but-never-created streams)
- CA-013: Docker port conflict (all partitions internal 3001)
- CA-020: ADR-002 covers only 2/12 streams
- ES-002: No circuit breaker on XADD producers
- ES-006: No lag monitoring (checkStreamLag not implemented)
- ES-010: DLQ overload synthesis

**MEDIUM (16):**
- LW-002, LW-006, LW-008, LW-009, LW-014, LW-015, LW-018, LW-019, LW-022, LW-023, LW-024
- CA-002, CA-005, CA-009, CA-012, CA-014

**LOW/INFO (11):**
- LW-007, LW-010, LW-016, LW-017, LW-025
- CA-004, CA-006, CA-007, CA-008, CA-015, CA-016

---

*Report generated by MONITORING.md v1.0*
*Session: `20260301_111756`*
*Completed: 2026-03-01T10:38:00Z*
*Agents: LOG_WATCHER (28 findings), CONFIG_AUDITOR (20 findings), ENHANCEMENT_SCOUT (13 findings), REDIS_STREAM_ANALYST (0 — helper issue)*
