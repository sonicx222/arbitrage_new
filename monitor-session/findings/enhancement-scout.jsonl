{"type":"heartbeat","agent":"ENHANCEMENT_SCOUT","at":"2026-03-01T10:30:00Z","status":"pass1_complete","findingsCount":8}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-001","category":"OBSERVABILITY_GAP","priority":"P2_MEDIUM","effort":"HOURS_2_4","impact":"MEDIUM","affectedServices":["partition-solana"],"affectedStreams":[],"pattern":"Console.log fallback in production logger","evidence":"services/partition-solana/src/arbitrage-detector.ts:257-259 - Logger defaults to console.log/warn/error when no logger provided in deps","recommendation":"Remove console fallback. Make logger required in constructor deps or use createLogger() from @arbitrage/core as default. Console output is unstructured and breaks observability tooling.","codeSnippetBefore":"this.logger = deps?.logger ?? {\n  info: console.log,\n  warn: console.warn,\n  error: console.error,\n  debug: () => {},\n};","codeSnippetAfter":"this.logger = deps?.logger ?? createLogger('solana-arbitrage-detector');"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-002","category":"STREAM_ANTI_PATTERN","priority":"P1_HIGH","effort":"HOURS_4_8","impact":"HIGH","affectedServices":["unified-detector","partition-asia-fast","partition-l2-turbo","partition-high-value","partition-solana"],"affectedStreams":["stream:opportunities"],"pattern":"XADD with MAXLEN has no circuit breaker protection on producers","evidence":"shared/core/src/redis/streams.ts:572-634 - xadd() has retry logic but no circuit breaker. During Redis outages, all partition services will hammer Redis with exponential backoff, creating a thundering herd on recovery.","recommendation":"Add circuit breaker protection to xaddWithLimit() and xadd(). When Redis fails, open circuit and fail fast instead of retry loops. This prevents cascading failures and reduces recovery time.","codeSnippetBefore":"async xadd<T>(\n  streamName: string,\n  message: T,\n  id: string = '*',\n  options: XAddOptions = {}\n): Promise<string> {\n  // ... retry logic but no circuit breaker\n  for (let attempt = 0; attempt <= (options.retry ? maxRetries : 0); attempt++) {\n    try {\n      return await this.client.xadd(...);\n    } catch (error) {\n      await this.sleep(Math.pow(2, attempt) * 100);\n    }\n  }\n}","codeSnippetAfter":"// Add circuit breaker field:\nprivate circuitBreaker = createSimpleCircuitBreaker({\n  threshold: 5,\n  resetTimeoutMs: 30000\n});\n\nasync xadd<T>(...): Promise<string> {\n  if (this.circuitBreaker.isOpen()) {\n    throw new Error('Redis circuit breaker open');\n  }\n  try {\n    const result = await this.client.xadd(...);\n    this.circuitBreaker.recordSuccess();\n    return result;\n  } catch (error) {\n    this.circuitBreaker.recordFailure();\n    throw error;\n  }\n}"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-003","category":"RESILIENCE","priority":"P2_MEDIUM","effort":"HOURS_2_4","impact":"MEDIUM","affectedServices":["coordinator","execution-engine"],"affectedStreams":["stream:dead-letter-queue"],"pattern":"Dead letter queue exists but no maxDelivery enforcement in StreamConsumer","evidence":"shared/core/src/redis/stream-consumer.ts:203-289 - StreamConsumer has retry via pending messages but no maxDelivery check. Failed messages stay in pending set indefinitely unless manually claimed/deleted. shared/core/src/resilience/dead-letter-queue.ts implements Redis-key-based DLQ, not stream-based.","recommendation":"Add maxDelivery config to StreamConsumerConfig. Track delivery count via XPENDING and move to DLQ after threshold. Example: count=XPENDING, if count > maxDelivery then XADD to DLQ and XACK original.","codeSnippetBefore":"export interface StreamConsumerConfig {\n  config: ConsumerGroupConfig;\n  handler: (message: StreamMessage) => Promise<void>;\n  batchSize?: number;\n  blockMs?: number;\n  autoAck?: boolean;\n  logger?: StreamConsumerLogger;\n}","codeSnippetAfter":"export interface StreamConsumerConfig {\n  config: ConsumerGroupConfig;\n  handler: (message: StreamMessage) => Promise<void>;\n  batchSize?: number;\n  blockMs?: number;\n  autoAck?: boolean;\n  logger?: StreamConsumerLogger;\n  maxDeliveryCount?: number; // Default 5, move to DLQ after this many failures\n  dlqStreamName?: string;     // DLQ stream name, defaults to DEAD_LETTER_QUEUE\n}"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-004","category":"PERFORMANCE","priority":"P3_LOW","effort":"HOURS_2_4","impact":"LOW","affectedServices":["coordinator","execution-engine"],"affectedStreams":["stream:opportunities","stream:execution-requests"],"pattern":"StreamConsumer defaults to COUNT=10 which may be suboptimal for high-throughput streams","evidence":"shared/core/src/redis/stream-consumer.ts:115 - Default batchSize is 10. For high-volume streams (opportunities, execution-requests), this creates excessive XREADGROUP calls. Coordinator processes ~1000 opp/sec in benchmarks, meaning 100 XREADGROUP/sec at default batch size.","recommendation":"Make batchSize configurable per stream type via env vars. Recommend COUNT=50 for opportunities, COUNT=100 for price updates. Document trade-off: higher batch size = better throughput, higher latency for last message in batch.","codeSnippetBefore":"constructor(client: RedisStreamsClient, config: StreamConsumerConfig) {\n  this.client = client;\n  this.config = {\n    batchSize: 10,\n    blockMs: 1000,\n    autoAck: true,\n    ...config\n  };\n}","codeSnippetAfter":"// Add env-based defaults:\nconst STREAM_BATCH_SIZES: Record<string, number> = {\n  [RedisStreamsClient.STREAMS.OPPORTUNITIES]: parseInt(process.env.OPP_BATCH_SIZE ?? '50', 10),\n  [RedisStreamsClient.STREAMS.PRICE_UPDATES]: parseInt(process.env.PRICE_BATCH_SIZE ?? '100', 10),\n  // ... other streams\n};\n\nconstructor(client: RedisStreamsClient, config: StreamConsumerConfig) {\n  const streamBatchSize = STREAM_BATCH_SIZES[config.config.streamName] ?? 10;\n  this.config = {\n    batchSize: streamBatchSize,\n    blockMs: 1000,\n    autoAck: true,\n    ...config\n  };\n}"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-005","category":"OBSERVABILITY_GAP","priority":"P2_MEDIUM","effort":"HOURS_4_8","impact":"HIGH","affectedServices":["all-partition-services","coordinator","execution-engine"],"affectedStreams":["all"],"pattern":"No StreamBatcher metrics exported to Prometheus","evidence":"shared/core/src/redis/streams.ts:133-142 - BatcherStats interface exists with compressionRatio, averageBatchSize, totalMessagesDropped, but no Prometheus export. These are critical for monitoring batching efficiency (target 50:1 per ADR-002) and detecting message loss.","recommendation":"Add Prometheus metrics endpoint for StreamBatcher stats. Export: batcher_messages_queued_total, batcher_messages_dropped_total, batcher_compression_ratio (gauge), batcher_avg_batch_size (gauge). Label by stream name. This enables alerting on low compression ratio (< 20:1) or dropped messages.","codeSnippetBefore":"// No Prometheus integration","codeSnippetAfter":"// In shared/core/src/redis/streams.ts, add:\nimport { Counter, Gauge, register } from 'prom-client';\n\nconst batcherMessagesQueued = new Counter({\n  name: 'redis_batcher_messages_queued_total',\n  help: 'Total messages queued in Redis Stream batcher',\n  labelNames: ['stream']\n});\n\nconst batcherMessagesDropped = new Counter({\n  name: 'redis_batcher_messages_dropped_total',\n  help: 'Total messages dropped due to queue full',\n  labelNames: ['stream']\n});\n\nconst batcherCompressionRatio = new Gauge({\n  name: 'redis_batcher_compression_ratio',\n  help: 'Batching compression ratio (messages queued / batches sent)',\n  labelNames: ['stream']\n});\n\n// Update stats in add() and flush() methods"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-006","category":"RESILIENCE","priority":"P1_HIGH","effort":"DAYS_1_2","impact":"HIGH","affectedServices":["coordinator","execution-engine","cross-chain-detector"],"affectedStreams":["stream:opportunities","stream:execution-requests","stream:pending-opportunities"],"pattern":"No lag monitoring for XREADGROUP consumers - risk of silent message loss","evidence":"shared/core/src/redis/streams.ts:640 comment: 'P0 Fix: Approximate MAXLEN can discard unread messages when consumers lag. Use checkStreamLag() in health checks to monitor lag.' However, checkStreamLag() is not implemented. StreamConsumer has no lag alerting.","recommendation":"Implement checkStreamLag() using XPENDING to measure consumer lag. Alert when lag > 80% of MAXLEN (e.g., 4000 pending on 5000 MAXLEN stream). Add to health checks in coordinator and execution-engine. This prevents silent message loss due to approximate MAXLEN trimming.","codeSnippetBefore":"// Comment exists but not implemented:\n// P0 Fix: Approximate MAXLEN can discard unread messages when consumers lag.\n// Use checkStreamLag() in health checks to monitor lag and alert before message loss.","codeSnippetAfter":"async checkStreamLag(streamName: string, groupName: string): Promise<{ pending: number; maxLen: number; lagPct: number; critical: boolean }> {\n  const pendingResult = await this.client.xpending(streamName, groupName);\n  const total = pendingResult[0]; // First element is total pending\n  const maxLen = RedisStreamsClient.STREAM_MAX_LENGTHS[streamName] ?? 10000;\n  const lagPct = (total / maxLen) * 100;\n  const critical = lagPct > 80;\n  \n  if (critical) {\n    this.logger.error('Consumer lag critical - risk of message loss', {\n      streamName,\n      groupName,\n      pending: total,\n      maxLen,\n      lagPct\n    });\n  }\n  \n  return { pending: total, maxLen, lagPct, critical };\n}"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-007","category":"OBSERVABILITY_GAP","priority":"P2_MEDIUM","effort":"HOURS_2_4","impact":"MEDIUM","affectedServices":["unified-detector","partition-services"],"affectedStreams":["stream:opportunities"],"pattern":"OpportunityPublisher has no correlation ID propagation from detection context","evidence":"shared/core/src/publishers/opportunity-publisher.ts:92 - Creates NEW trace context on publish instead of propagating detector's context. This breaks end-to-end tracing from price update → detection → coordinator → execution.","recommendation":"Change OpportunityPublisher.publish() to accept optional traceContext parameter. When provided, use propagateContext() with existing context instead of createTraceContext(). Update detector to pass its trace context through.","codeSnippetBefore":"async publish(opportunity: ArbitrageOpportunity): Promise<boolean> {\n  const sourceName = `${this.sourcePrefix}-${this.partitionId}`;\n  const traceCtx = createTraceContext(sourceName); // Creates NEW context\n  const enrichedOpportunity = propagateContext({\n    ...opportunity,\n    _source: sourceName,\n    _publishedAt: Date.now(),\n  }, traceCtx);\n}","codeSnippetAfter":"async publish(opportunity: ArbitrageOpportunity, parentTraceContext?: TraceContext): Promise<boolean> {\n  const sourceName = `${this.sourcePrefix}-${this.partitionId}`;\n  // Use parent context if provided, otherwise create new\n  const traceCtx = parentTraceContext ?? createTraceContext(sourceName);\n  const enrichedOpportunity = propagateContext({\n    ...opportunity,\n    _source: sourceName,\n    _publishedAt: Date.now(),\n  }, traceCtx);\n}"}
{"agentId":"ENHANCEMENT_SCOUT","findingId":"ES-008","category":"DX_IMPROVEMENT","priority":"P3_LOW","effort":"HOURS_2_4","impact":"LOW","affectedServices":["all-services"],"affectedStreams":["all"],"pattern":"Stream names are constants but not type-safe at call sites","evidence":"shared/core/src/redis/streams.ts:106 - xaddWithLimit() accepts string streamName. Callers can pass typos like 'stream:oportunities' instead of RedisStreamsClient.STREAMS.OPPORTUNITIES. This compiles but fails at runtime.","recommendation":"Add StreamName type union from STREAMS keys. Change method signatures to accept StreamName instead of string. TypeScript will enforce valid stream names at compile time.","codeSnippetBefore":"async xaddWithLimit<T = Record<string, unknown>>(\n  streamName: string,\n  message: T,\n  options: Omit<XAddOptions, 'maxLen'> = {}\n): Promise<string>","codeSnippetAfter":"// Add type at top of file:\nexport type StreamName = typeof RedisStreamsClient.STREAMS[keyof typeof RedisStreamsClient.STREAMS];\n\n// Update signature:\nasync xaddWithLimit<T = Record<string, unknown>>(\n  streamName: StreamName,\n  message: T,\n  options: Omit<XAddOptions, 'maxLen'> = {}\n): Promise<string>"}
{"type":"heartbeat","agent":"ENHANCEMENT_SCOUT","at":"2026-03-01T10:35:00Z","status":"pass1_findings_written","findingsCount":8}
{"type":"pass2_start","agent":"ENHANCEMENT_SCOUT","at":"2026-03-01T10:40:00Z","status":"cross_agent_analysis"}
{"type":"pass2_complete","agent":"ENHANCEMENT_SCOUT","at":"2026-03-01T10:45:00Z","status":"complete","totalFindings":13,"synthesisFindings":5}
